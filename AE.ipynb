{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adjacency import Adjacency, generate_random_genome\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for this dataset, the implemented model failed to obtain non-zero accuracy. In order to accurately reproduce the genome from the adjacency matrix, a considerable number of kmer is required, due to which the training data simply does not fit into the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_fasta_file(file_path):\n",
    "#     sequences = {}  # Dictionary to store sequences\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         current_header = None\n",
    "#         current_sequence = ''\n",
    "#         for line in f:\n",
    "#             line = line.strip()  # Remove leading/trailing whitespace\n",
    "#             if line.startswith('>'):\n",
    "#                 # If the line starts with '>', it's a header line\n",
    "#                 # Store the previous sequence (if any) and update current header\n",
    "#                 if current_header is not None:\n",
    "#                     sequences[current_header] = current_sequence\n",
    "#                 current_header = line[1:]  # Remove '>'\n",
    "#                 current_sequence = ''\n",
    "#             else:\n",
    "#                 # If the line doesn't start with '>', it's sequence data\n",
    "#                 current_sequence += line\n",
    "#         # Store the last sequence\n",
    "#         sequences[current_header] = current_sequence\n",
    "#     return sequences\n",
    "\n",
    "# file_path = 'genome.txt' \n",
    "# sequences = read_fasta_file(file_path)\n",
    "\n",
    "# random.seed(42)\n",
    "\n",
    "# # Function to replace 'N' with a random letter from 'ACGT'\n",
    "# def replace_N_with_random_letter(string):\n",
    "#     return ''.join(random.choice('ACGT') if char == 'N' else char for char in string)\n",
    "\n",
    "# # Update dictionary values\n",
    "# for key, value in sequences.items():\n",
    "#     sequences[key] = replace_N_with_random_letter(value)\n",
    "\n",
    "# genomes = list(sequences.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "randomly generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomes = [generate_random_genome(5) for i in range(1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting genomes into adjacency matrices and testing accuracy genome -> matrix -> genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_mer_len = 3\n",
    "matrix_size = 4**k_mer_len\n",
    "adj = Adjacency(k_mer_len)\n",
    "adj_genomes = [adj.genome2adjacency(i) for i in genomes]\n",
    "genomes_adj = [adj.adjacency2genome(i[0], i[1]) for i in adj_genomes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors: 13/1000\n"
     ]
    }
   ],
   "source": [
    "errors=0\n",
    "for i in range(1000):\n",
    "    if genomes[i] != genomes_adj[i]:\n",
    "        errors +=1\n",
    "print(f\"errors: {errors}/{1000}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model consists of encoder and decoder with 3 linear layers in each. I used MSE loss function and rmsprop optimizer, also tried adam and adagrad, but they did no better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I read a little about how people trained encoders for sparse matrices, they used different regularization methods, custom loss functions, etc. https://www.quora.com/When-training-an-autoencoder-on-very-sparse-data-how-do-you-force-the-decoder-to-reconstruct-mostly-zeros-rather-than-always-just-reconstructing-the-average. but I did not find any meaningful code for that\n",
    "\n",
    "There is an article https://arxiv.org/pdf/1905.03375 where the authors made an autoencoder for sparse matrices for recommender systems, in the abstract it was said that their simple linear model has better results than complex nonlinear ones. there is a code on github, I think it will be possible to try to figure out how to apply it to our data and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have found that the following things help to increase accuracy: \n",
    "\n",
    "removing ReLUs between hidden layers\n",
    "\n",
    "decreasing learning rate\n",
    "\n",
    "the shorter the length of the genome, the greater the accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the genome with length 5 the following results were observed:\n",
    "\n",
    "with RelUs, 3-mers acc = 0.3\n",
    "\n",
    "with RelUs, 2-mers acc = 0.11\n",
    "\n",
    "with RelUs, 1-mers acc = 0.03\n",
    "\n",
    "without ReLUs between hidden layers, 3-mers acc = 0.34\n",
    "\n",
    "removed last linear layer from encoder and first from decoder, without ReLUs between hidden layers, 3-mers acc = 0.27\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the accuracy on the training data is quite low, I didn't make train/test split. I calculated the accuracy by comparing the adjacency matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, matrices):\n",
    "        self.matrices = matrices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.matrices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.matrices[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, matrix_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.matrix_size = matrix_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(matrix_size*matrix_size, matrix_size*4),  \n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(matrix_size*4, matrix_size),  \n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(matrix_size, matrix_size//2),  \n",
    "            # nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            # nn.Linear(matrix_size//2, matrix_size),  \n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(matrix_size, matrix_size*4),  \n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(matrix_size*4, matrix_size*matrix_size),\n",
    "            nn.ReLU()  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.matrix_size*self.matrix_size)\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        x = x.view(-1, self.matrix_size, self.matrix_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i tried to normalize matrices, but it had no effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrices_normalized = [(matrix[0] - np.min(matrix[0])) / (np.max(matrix[0]) - np.min(matrix[0])) for matrix in adj_genomes]\n",
    "matrices_normalized = [matrix[0] for matrix in adj_genomes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also tried lr scheduler, it didn't help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maksym/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder(matrix_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.00001, alpha=0.9)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# optimizer = optim.Adagrad(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "dataset = MyDataset(matrices_normalized)\n",
    "dataloader = DataLoader(dataset, batch_size=50, shuffle=True)\n",
    "testloader = DataLoader(dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.0004754977024276741\n",
      "Accuracy: 0/1000\n",
      "Epoch 200, Loss: 0.0004429528024047613\n",
      "Accuracy: 0/1000\n",
      "Epoch 300, Loss: 0.0003862620840664022\n",
      "Accuracy: 12/1000\n",
      "Epoch 400, Loss: 0.0003225998720154166\n",
      "Accuracy: 109/1000\n",
      "Epoch 500, Loss: 0.00027580811220104804\n",
      "Accuracy: 194/1000\n",
      "Epoch 600, Loss: 0.00025307465839432555\n",
      "Accuracy: 246/1000\n",
      "Epoch 700, Loss: 0.0002446484912070446\n",
      "Accuracy: 267/1000\n",
      "Epoch 800, Loss: 0.00024200209372793324\n",
      "Accuracy: 267/1000\n",
      "Epoch 900, Loss: 0.00024153721678885632\n",
      "Accuracy: 268/1000\n",
      "Epoch 1000, Loss: 0.00024146393770934081\n",
      "Accuracy: 268/1000\n",
      "Epoch 1100, Loss: 0.00024145515635609627\n",
      "Accuracy: 268/1000\n",
      "Epoch 1200, Loss: 0.000241455074865371\n",
      "Accuracy: 268/1000\n",
      "Epoch 1300, Loss: 0.0002414550785033498\n",
      "Accuracy: 268/1000\n",
      "Epoch 1400, Loss: 0.0002414550763205625\n",
      "Accuracy: 268/1000\n",
      "Epoch 1500, Loss: 0.00024145507995854132\n",
      "Accuracy: 268/1000\n",
      "Epoch 1600, Loss: 0.00024145507559296674\n",
      "Accuracy: 268/1000\n",
      "Epoch 1700, Loss: 0.0002414550763205625\n",
      "Accuracy: 268/1000\n",
      "Epoch 1800, Loss: 0.00024145507413777522\n",
      "Accuracy: 268/1000\n",
      "Epoch 1900, Loss: 0.00024145507413777522\n",
      "Accuracy: 268/1000\n",
      "Epoch 2000, Loss: 0.00024145507413777522\n",
      "Accuracy: 268/1000\n",
      "Epoch 2100, Loss: 0.0002414550763205625\n",
      "Accuracy: 268/1000\n",
      "Epoch 2200, Loss: 0.00024145507777575403\n",
      "Accuracy: 268/1000\n",
      "Epoch 2300, Loss: 0.00024145507704815828\n",
      "Accuracy: 268/1000\n",
      "Epoch 2400, Loss: 0.00024145507777575403\n",
      "Accuracy: 268/1000\n",
      "Epoch 2500, Loss: 0.00024145507413777522\n",
      "Accuracy: 268/1000\n",
      "Epoch 2600, Loss: 0.00024145507777575403\n",
      "Accuracy: 268/1000\n",
      "Epoch 2700, Loss: 0.000241455074865371\n",
      "Accuracy: 268/1000\n",
      "Epoch 2800, Loss: 0.00024145507777575403\n",
      "Accuracy: 268/1000\n",
      "Epoch 2900, Loss: 0.000241455074865371\n",
      "Accuracy: 268/1000\n",
      "Epoch 3000, Loss: 0.00024145507704815828\n",
      "Accuracy: 268/1000\n",
      "Epoch 3100, Loss: 0.00024145507559296674\n",
      "Accuracy: 268/1000\n",
      "Epoch 3200, Loss: 0.000241455074865371\n",
      "Accuracy: 268/1000\n",
      "Epoch 3300, Loss: 0.000241455074865371\n",
      "Accuracy: 268/1000\n",
      "Epoch 3400, Loss: 0.00024145507413777522\n",
      "Accuracy: 268/1000\n",
      "Epoch 3500, Loss: 0.00024145507559296674\n",
      "Accuracy: 268/1000\n",
      "Epoch 3600, Loss: 0.0002414550763205625\n",
      "Accuracy: 268/1000\n",
      "Epoch 3700, Loss: 0.00024145507704815828\n",
      "Accuracy: 268/1000\n",
      "Epoch 3800, Loss: 0.00024145507341017948\n",
      "Accuracy: 268/1000\n",
      "Epoch 3900, Loss: 0.00024145507704815828\n",
      "Accuracy: 268/1000\n",
      "Epoch 4000, Loss: 0.0002414550763205625\n",
      "Accuracy: 268/1000\n",
      "Epoch 4100, Loss: 0.00024145507559296674\n",
      "Accuracy: 268/1000\n",
      "Epoch 4200, Loss: 0.00024145507559296674\n",
      "Accuracy: 268/1000\n",
      "Epoch 4300, Loss: 0.0002414550763205625\n",
      "Accuracy: 268/1000\n",
      "Epoch 4400, Loss: 0.0002414550763205625\n",
      "Accuracy: 268/1000\n",
      "Epoch 4500, Loss: 0.00024145507559296674\n",
      "Accuracy: 268/1000\n",
      "Epoch 4600, Loss: 0.00024145507777575403\n",
      "Accuracy: 268/1000\n",
      "Epoch 4700, Loss: 0.00024145507559296674\n",
      "Accuracy: 268/1000\n",
      "Epoch 4800, Loss: 0.0002414550785033498\n",
      "Accuracy: 268/1000\n",
      "Epoch 4900, Loss: 0.00024145507704815828\n",
      "Accuracy: 268/1000\n",
      "Epoch 5000, Loss: 0.0002414550763205625\n",
      "Accuracy: 268/1000\n",
      "Epoch 5100, Loss: 0.00024145507559296674\n",
      "Accuracy: 268/1000\n",
      "Epoch 5200, Loss: 0.0002414550763205625\n",
      "Accuracy: 268/1000\n",
      "Epoch 5300, Loss: 0.00024145507704815828\n",
      "Accuracy: 268/1000\n",
      "Epoch 5400, Loss: 0.0002414550806861371\n",
      "Accuracy: 268/1000\n",
      "Epoch 5500, Loss: 0.0002414550763205625\n",
      "Accuracy: 268/1000\n",
      "Epoch 5600, Loss: 0.000241455074865371\n",
      "Accuracy: 268/1000\n",
      "Epoch 5700, Loss: 0.00024145507413777522\n",
      "Accuracy: 268/1000\n",
      "Epoch 5800, Loss: 0.00024145507704815828\n",
      "Accuracy: 268/1000\n",
      "Epoch 5900, Loss: 0.0002414550763205625\n",
      "Accuracy: 268/1000\n",
      "Epoch 6000, Loss: 0.000241455074865371\n",
      "Accuracy: 268/1000\n",
      "Epoch 6100, Loss: 0.00024145507777575403\n",
      "Accuracy: 268/1000\n",
      "Epoch 6200, Loss: 0.00024145507559296674\n",
      "Accuracy: 268/1000\n",
      "Epoch 6300, Loss: 0.0002414550785033498\n",
      "Accuracy: 268/1000\n",
      "Epoch 6400, Loss: 0.0002414550763205625\n",
      "Accuracy: 268/1000\n",
      "Epoch 6500, Loss: 0.0002414550763205625\n",
      "Accuracy: 268/1000\n",
      "Epoch 6600, Loss: 0.000241455074865371\n",
      "Accuracy: 268/1000\n",
      "Epoch 6700, Loss: 0.00024145507777575403\n",
      "Accuracy: 268/1000\n",
      "Epoch 6800, Loss: 0.00024145507341017948\n",
      "Accuracy: 268/1000\n",
      "Epoch 6900, Loss: 0.000241455074865371\n",
      "Accuracy: 268/1000\n",
      "Epoch 7000, Loss: 0.00024145507704815828\n",
      "Accuracy: 268/1000\n",
      "Epoch 7100, Loss: 0.000241455074865371\n",
      "Accuracy: 268/1000\n",
      "Epoch 7200, Loss: 0.0002414550785033498\n",
      "Accuracy: 268/1000\n",
      "Epoch 7300, Loss: 0.00024145507704815828\n",
      "Accuracy: 268/1000\n",
      "Epoch 7400, Loss: 0.0002414550763205625\n",
      "Accuracy: 268/1000\n",
      "Epoch 7500, Loss: 0.00024145507704815828\n",
      "Accuracy: 268/1000\n",
      "Epoch 7600, Loss: 0.00024145507777575403\n",
      "Accuracy: 268/1000\n",
      "Epoch 7700, Loss: 0.00024145507923094555\n",
      "Accuracy: 268/1000\n",
      "Epoch 7800, Loss: 0.00024145507777575403\n",
      "Accuracy: 268/1000\n",
      "Epoch 7900, Loss: 0.00024145507777575403\n",
      "Accuracy: 268/1000\n",
      "Epoch 8000, Loss: 0.00024145507559296674\n",
      "Accuracy: 268/1000\n",
      "Epoch 8100, Loss: 0.00024145507777575403\n",
      "Accuracy: 268/1000\n",
      "Epoch 8200, Loss: 0.00024145507923094555\n",
      "Accuracy: 268/1000\n",
      "Epoch 8300, Loss: 0.00024145507413777522\n",
      "Accuracy: 268/1000\n",
      "Epoch 8400, Loss: 0.0002414550763205625\n",
      "Accuracy: 268/1000\n",
      "Epoch 8500, Loss: 0.00024145507777575403\n",
      "Accuracy: 268/1000\n",
      "Epoch 8600, Loss: 0.00024145507559296674\n",
      "Accuracy: 268/1000\n",
      "Epoch 8700, Loss: 0.00024145507777575403\n",
      "Accuracy: 268/1000\n",
      "Epoch 8800, Loss: 0.00024145507704815828\n",
      "Accuracy: 268/1000\n",
      "Epoch 8900, Loss: 0.00024145507777575403\n",
      "Accuracy: 268/1000\n",
      "Epoch 9000, Loss: 0.0002414550763205625\n",
      "Accuracy: 268/1000\n",
      "Epoch 9100, Loss: 0.0002414550763205625\n",
      "Accuracy: 268/1000\n",
      "Epoch 9200, Loss: 0.0002414550785033498\n",
      "Accuracy: 268/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      5\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, data)\n\u001b[1;32m      8\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m, in \u001b[0;36mAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatrix_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatrix_size)\n\u001b[0;32m---> 24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x)\n\u001b[1;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatrix_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatrix_size)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    # Print loss every 20 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader)}\")\n",
    "        \n",
    "        # Evaluation\n",
    "        all_predictions = []\n",
    "        model.eval()\n",
    "        for data in testloader:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(data)\n",
    "            all_predictions.append(outputs.numpy()[0])\n",
    "        int_predictions = [np.round(matrix).astype(int) for matrix in all_predictions]\n",
    "        errors = 0\n",
    "        for i in range(len(matrices_normalized)):\n",
    "            if not np.array_equal(int_predictions[i], matrices_normalized[i]):\n",
    "                errors += 1\n",
    "        print(f\"Accuracy: {len(matrices_normalized)-errors}/{len(matrices_normalized)}\")\n",
    "    # scheduler.step()\n",
    "        # print(f\"Error Rate: {errors / len(matrices_normalized)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
